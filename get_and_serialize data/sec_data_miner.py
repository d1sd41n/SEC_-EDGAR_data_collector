""" The purpose of this library is to help us
 with the search and collection of financial data from
 the SEC's EDGAR database. """
import requests
from bs4 import BeautifulSoup
from os import listdir
from os.path import isfile, join
import os
import urllib.request
from os import listdir
from os.path import isfile, join
import pandas as pd
import shutil # this library will be used to remove folders
import re

class Sec_data:
    def __init__(self, CIK="1018724", type_='10-Q'):
        # define our parameters dictionary
        self.CIK = CIK
        self.param_dict = {
                'action':'getcompany',
                'CIK':CIK,
                'type':type_,
                'dateb':'',
                'owner':'exclude',
                'start':'',
                'output':'',
                'count':'100'
                }

    def get_fin_docs(self):
        """
        # 1 #
        This method gets all 1-Q interactive data URL from
        one company.
        
        Output:
            *I_D_links: returns a list of lists with all data URLs:
                the sub list has:
                    *index 0: url to interactive data.
                    *index 1: date of publication of the data
                    *index 2: url to excel doc of the interactive data
        """
        # base URL for the SEC EDGAR browser
        endpoint = r"https://www.sec.gov/cgi-bin/browse-edgar"

        # request the url, and then parse the response.
        response = requests.get(url = endpoint, params = self.param_dict)
        soup = BeautifulSoup(response.content, 'lxml')

        # print status code
        print("Search page URL: " + response.url)

        # this list will save all interative data links
        I_D_links = []

        # get the table that have the class tableFile2
        table = soup.find(class_="tableFile2")
        rows = table.find_all("tr")[1:] #removes the header row

        # iterates al rows of the table
        for row in rows:
            l = []
            s = "interactiveDataBtn"

            # if interactiveDataBtn text exist in the row
            if s in row.decode():
                date = row.find_all("td")[3].string # get the date
                url = row.find(id=s).get('href') # get the interactive data url
                l.append("https://www.sec.gov" + url)
                l.append(date)
                I_D_links.append(l)

        # getting the xls finantial docs
        len_docs = len(I_D_links)
        print("Gettings docs URLs... ")
        for n, l in enumerate(I_D_links):
            print(str(int(100*n/len_docs)) + "%...") # print the percentage of the process
            endpoint = l[0]
            response = requests.get(url = endpoint)
            soup = BeautifulSoup(response.content, 'lxml')
            # look for the xls docs links
            entries = soup.find("a", string="View Excel Document")
            url = entries.get('href')
            I_D_links[n].append("https://www.sec.gov" + url)
        print("done!...")
        return I_D_links

    def download_fin_files(self, data):
        """ 
        # 2 #
        download all financial reports available in excel of a company
        input:
           *data: is alist with the URLs of the 10-q interactive data of a company, 
                  this list is generated by the method: get_fin_docs()
        output:
            *path: this function returns the name of the folder where,
            the files have been downloaded.
            """
        # name of the folder where docs will be downloaded
        path = '{}_finantial_docs'.format(self.CIK)

        # if the folder exist will be deleted and created again
        if not os.path.exists(path):
            os.makedirs(path)
        else:
            shutil.rmtree(path, ignore_errors=True)
            os.makedirs(path)

        # downloading all files and saving it in a folder
        print("Downloading documents of {} company:".format(self.CIK))
        len_data = len(data)
        for i, d in enumerate(data):
            print(str(int(100*i/len_data)) + "%...") # print the percentage of the process
            url = d[2]
            name = "{}/{}.xlsx".format(path, d[1]) # creates the file using the date
            urllib.request.urlretrieve(url, name)
        print("done!...")
        print("the files of company: {} have been downloaded in: {}".
               format(self.CIK, path))
        return path

        # getting the xls finantial docs
        len_docs = len(I_D_links)
        print("Gettings docs URLs... ")
        for n, l in enumerate(I_D_links):
            print(str(int(100*n/len_docs)) + "%...") # print the percentage of the process
            endpoint = l[0]
            response = requests.get(url = endpoint)
            soup = BeautifulSoup(response.content, 'lxml')
            # look for the xls docs links
            entries = soup.find("a", string="View Excel Document")
            url = entries.get('href')
            I_D_links[n].append("https://www.sec.gov" + url)
        print("done!...")
        return I_D_links

    def sort_and_clean_finfiles(self, path):
        """" 
        # 3 #
        get all files in the folder given in the path and 
        removes the corrupted files and returns an organized 
        list with the file names. File names are dates by which they will be sorted from 
        the most recent date to the oldest. This function depends on 
        the 'download_fin_files' function.

        input:
            *path: the path to the folder where the finantial files are.
        ouput:
            *files_cleaned: returns a name-date ordered non-corrupted files.
        """
        # get all file names in the folder in a list
        # path = "finantial_docs"
        files = [f for f in listdir(path) if isfile(join(path, f))]

        # sort all files in the folder
        files.sort(reverse=True)

        # lets see all len of sheets of all files
        # and creates a new file list wihout corrupte files
        files_cleaned = []
        corrupted_files = []
        for n, file in enumerate(files):
        # for file in files:
            print("------------------")
            print("Index: {}\nFile: {}".format(n, file))
            try:
                doc = pd.ExcelFile("{}/{}".format(path, file))
                files_cleaned.append(file)
                print("sheets: {}".format(len(doc.sheet_names)))
            except:
                print("{}: is corrupted".format(file))
                os.remove("{}/{}".format(path, file))
                print("File deleted!")
                corrupted_files.append(file)
        print("Done!...")
        return files_cleaned

    def extract_and_sort_data(self, files, path):
        """
        # 4 #
        This function extracts the main financial data from the
        files and organizes them in inversed chronological
        order in a dataframe and a dict
        
        input:
            * files: a list with all the names of the
            files whose dates are arranged in reverse chronological
            order, this list returned by the sort_and_clean_finfiles method
            * path: a string with a path where the files are, this variable 
            must be the same one that receives the sort_and_clean_finfiles method
        output:
            * returns a list with two elements, in position 0 a dataframe
            with all the data organized and in position 1 a dictionary
            with all the elements organized"""

        #############################################
        # patterns of statements of operations sheet
        #############################################

        # pattern to find statement of operation in all sheets of the file
        pattern_s_oper = re.compile(r'statement(s|)[_\s]of[_\s](oper|Ope)', re.IGNORECASE)

        # patt fields to search within Statements of Operations sheet
        pattern_net_sales = re.compile(r'net[_\s]sales', re.IGNORECASE)
        pattern_net_income = re.compile(r'net[_\s]income', re.IGNORECASE)

        # list with all patts to find fields within Statements of Operations
        s_oper_patt_list = [pattern_net_sales, pattern_net_income]

        ##################################
        # patterns of balance sheet
        ##################################

        # pattern to find balance sheet in all sheets of the file
        pattern_balan_sheet = re.compile(r'balance[_\s]sheet(s|)', re.IGNORECASE)

        # patt fields to search within balance sheet
        pattern_cash_equi = re.compile(r'cash[_\s]and[_\s]cash[_\s]equivalents', re.IGNORECASE)
        pattern_inventory = re.compile(r'inventor(y|ies)', re.IGNORECASE)
        pattern_total_assets = re.compile(r'total[_\s]current[_\s]assets', re.IGNORECASE)
        pattern_accnt_pay = re.compile(r'accounts[_\s]payable', re.IGNORECASE)
        pattern_long_debt = re.compile(r'long-term[_\s]debt', re.IGNORECASE)
        pattern_total_liabts = re.compile(r'total[_\s]current[_\s]liabilities', re.IGNORECASE)

        # list with all patts to find fields within balance sheet
        balan_sheet_patt_list = [pattern_cash_equi, pattern_inventory,
                                pattern_total_assets, pattern_accnt_pay,
                                pattern_long_debt, pattern_total_liabts]

        # list containing the patterns to find sheets
        patterns_sheets = [pattern_s_oper, pattern_balan_sheet]

        # This dictionary stores all the data of the searched fields within each requested sheet, 
        # the keys will be the patterns used to search the requested fields
        # and a key dates to store each date extracted from the name of each file
        data_dict = {pattern_net_sales: [], pattern_net_income: [],
                    pattern_cash_equi: [], pattern_inventory: [],
                    pattern_total_assets: [], pattern_accnt_pay: [],
                    pattern_long_debt: [], pattern_total_liabts: [],
                    "dates": []}
        #######################################################################
        # in this part data is extracted from the files, it will iterate over each file,
        # sheet, patterns_sheets, field pattern, sheet data df columns

        print("***************************************************")
        print("Searching and extracting data!")
        print("***************************************************\n\n")

        for n, file in enumerate(files): # iterating files------------------------
            print("#################################")
            print("Index: {}\nFile: {}".format(n, file))
            doc = pd.ExcelFile("{}/{}".format(path, file)) # get the file using pandas
            sheets = doc.sheet_names # get all sheets of file
            patterns_sheets_copy = patterns_sheets[:] # get a copy of sheets
            # this dict constains a list with copy list of the patterns fields for each sheet
            patterns_dict = {pattern_s_oper: s_oper_patt_list[:],
                            pattern_balan_sheet: balan_sheet_patt_list[:]}
            for n, sheet in enumerate(sheets): # iterating sheets of file-------------

                for i, pattern in enumerate(patterns_sheets_copy): # iterate sheets patterns-----
                    matches = pattern.finditer(sheet) # look for the sheet comparing it with the pattern
                    patt_sheet_ = [match for match in matches]  # enters the search results in the list

                    if len(patt_sheet_) > 0: # if we found a sheet pattern
                        print("------------------")
                        print("Sheet:", sheet)
                        patterns_sheets_copy.pop(i) # remove the pattern from the list when it's found
                        df = pd.read_excel(doc, sheet) # get df from the sheet
                        df.set_index(df.columns[0], inplace=True) # set column 0 as index
                        df = df.T # transpose the df
                        columns = df.columns # get the colums of the df
                        sheet_field_patts = patterns_dict[pattern] # get the patterns field list o the current sheet

                        for j, patt_field in enumerate(sheet_field_patts): # iterating patts of fields---------------------

                            for col in df.columns: # iterating the columns of the df----------------

                                # with this we avoid that the key "dates" which is string causes errors
                                if type(col) != type('str'):
                                    continue

                                patt = patt_field.finditer(col) # look for the field comparing it with the pattern
                                patt_column_ = [match for match in patt]

                                if len(patt_column_) > 0:  # if we found a shfield pattern
                                    val = df[[col]].values[0][0] # get the val of that colum
                                    date = file.split(".")[0]  # enters the search results in the list
                                    if not date in data_dict["dates"]: # save the date in data_dict
                                        data_dict["dates"].append(date)
                                    data_dict[patt_field].append(val) #saves the field in data_dict
                                    print("{}: {}".format(col, val))
                                    break

        # this helps to visualize if all the lists in all the fields are the same length
        # for key in data_dict:
        #     print(key)
        #     print(len(data_dict[key]))
        ##########################################################################
        # this list has the names of the columns of the new dataframe
        columns = ["net sales", "net income",
                "cash and cash equivalents",
                "inventories", "total current assets",
                "accounts payable", "long-term debt",
                "total current liabilities"]
        # In this dictionary all the data of data_dict will be stored replacing
        # the keys that were the patters with the names of the columns
        finantial_dict = {}

        for key in data_dict.keys():
            if type(key) == type('str'):
                continue
            for col in columns:
                patt = key.finditer(col)
                col_patt =[x for x in patt]
                if len(col_patt) > 0:
                    col_name = col_patt[0].string
                    finantial_dict[col_name] = data_dict[key]
                    break
        # inserts the dates in the dictionary
        finantial_dict["dates"] = data_dict["dates"]

        ###############################################################
        # the dataframe is created using the new dictionary
        df = pd.DataFrame(data=finantial_dict)
        # convert dates from string to datetime format
        df['dates']= pd.to_datetime(df['dates'])
        # sets dates column as index of df
        df.set_index("dates", inplace=True)
        print("\n\n***************************************************")
        print("process completed successfully!")
        print("***************************************************")
        
        return [df, finantial_dict]
